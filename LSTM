import numpy as np
import pandas as pd
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import re
import string
import tensorflow
from tensorflow.keras import layers
from keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from sklearn import metrics
import os
import pickle
from sklearn.utils import shuffle

import nltk
nltk.download("wordnet")
nltk.download("stopwords")
nltk.download("punkt")
nltk.download("omw-1.4")

#T5 VEYA PEGASUS İLE ÜRETİLMİŞ VERİ SETİ
augmented_dataset.rename(columns={'Text': 'Message', 'Label': 'Category'}, inplace=True)

#ORJİNAL VERİ SETİ
df.rename(columns={'Text': 'Message', 'Label': 'Category'}, inplace=True)


#BAŞTA
df_top=pd.concat([augmented_dataset,df],ignore_index=True)
df_top.count()

#SONDA
df_top=pd.concat([df,augmented_dataset],ignore_index=True)
df_top.count()

#KARIŞIK
df_top=pd.concat([df,augmented_dataset],ignore_index=True)
df_top = shuffle(df_top)

#ORTADA
df_top = pd.concat([df.iloc[:250], augmented_dataset, df.iloc[250:]]).reset_index(drop=True)


#DUPLİACTED OLANLARIN SİLİNMESİ
len(df_top) - len(df_top.drop_duplicates())

df_top.drop_duplicates(inplace = True)

#ÖN İŞLEME
def preprocess(text):
    text = text.lower()
    text = re.sub(r'https?://\S+|www.\S+', '', text)
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'[^a-zA-Z]+', ' ', text)
    text = re.sub(r'[0-9]', '', text)
    translator = str.maketrans("", "", string.punctuation)
    text = text.translate(translator)
    words = word_tokenize(text)
    words = [word for word in words if word not in stopwords.words("english")]
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]
    text = " ".join(words)
    return text

#ÖN İŞLEME-2
df_top["Message"] = df_top.Message.map(preprocess) 

#ÖN İŞLEME-3   
maxSeq = len(df_top["Message"][0])
for i in range (0,len(df_top["Message"])):
    try:
        cur = len(df_top["Message"][i])
        if (cur > maxSeq):
            maxSeq = cur
    except:
        pass

#ÖN İŞLEME-4        
from collections import Counter

def counter_word(text_col):
    count = Counter()
    for text in text_col.values:
        for word in text.split():
            count[word] += 1
    return count

counter = counter_word(df_top.Message)

MAX_NB_WORDS = len(counter)
MAX_SEQUENCE_LENGTH = maxSeq

#ÖN İŞLEME-5
EMBEDDING_DIM = 100
tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(df_top['Message'].values)
with open("tokenizer.pickle", "wb") as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
    
#ÖN İŞLEME-6    
X = tokenizer.texts_to_sequences(df_top['Message'].values)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)
print('Shape of data tensor:', X.shape)

#ÖN İŞLEME-7
from sklearn import preprocessing
labelencoder = preprocessing.LabelEncoder()
df_top['Category'] = labelencoder.fit_transform(df_top['Category'])
Y = df_top['Category'].values
print('Shape of label tensor:', Y.shape)

#TRAIN TEST SPLIT
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.25, random_state = 42)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

#MODEL KURULMASI
import tensorflow 
from tensorflow.keras import layers
from keras.callbacks import ModelCheckpoint, EarlyStopping


model = tensorflow.keras.models.Sequential()
model.add(layers.Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(layers.LSTM(32))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

epochs = 5
batch_size = 64

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])


print(history.model.summary())

#TRAIN LOSS TEST LOSS DEĞERLERİNİN GÖRSELLEŞTİRİLMESİ
plt.title('Loss')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show()

#ACCURACY DEĞERLERİNİN GÖRSELLEŞTİRİLMESİ
plt.title('Accuracy')
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show()


#PREDICT
threshold = 0.5

result = model.predict(X_test, verbose=2)
result = result > threshold
result = result.astype(int)

#KARMAŞIKLIK MATRİSİ
from sklearn.metrics import classification_report
target_names = ['ham','spam']
print(classification_report(Y_test, result, target_names=target_names))

   
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(Y_test, result)
cm_df = pd.DataFrame(cm,
                     index = target_names, 
                     columns = target_names)
cm_df

#AUC DEĞERİ
from sklearn import metrics
print("AUC:",metrics.roc_auc_score(Y_test, result))
cutoff_grid = np.linspace(0.0,1.0,100)
TPR = []
FPR = []
cutoff_grid
FPR, TPR, cutoffs = metrics.roc_curve(Y_test, result,pos_label=1)
plt.plot(FPR,TPR,c='red',linewidth=1.0)
plt.xlabel('False Positive')
plt.ylabel('True Positive')
plt.title('ROC Curve')
plt.show()

